# 激活函数

### 0x01. 激活函数的性质

- **非线性：** 为模型引入非线性因素
- **几乎处处可微：** 有限的不可微点有左右导数（左右导数可能不同，如Relu）。 便于反向传播，利于优化
- **计算简单：** 激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。这也是ReLU比其它使用Exp等操作的激活函数更受欢迎的其中一个原因。
- **非饱和性：** 饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。
- **单调性：** 当激活函数是单调的时候，单层网络能够保证是凸函数；



### 0x02. 常见激活函数

#### 1. Sigmoid函数

![img](https://pic2.zhimg.com/80/v2-309f1d5e5b95bd8a2de8c5d1f065297d_1440w.jpg)

特点是输出分布在[0,1]区间。

缺点：

- 在死区会导致梯度消失；
- 函数输出**不是以 0 为中心的**，这会降低权重更新的效率；
- Sigmoid 函数执行指数运算，运行得较慢。

#### 2. Tanh函数

!![img](https://pic3.zhimg.com/80/v2-d04be8777f2eeba6321d90d9d3106d8e_1440w.jpg)

特点：

- 并且整个函数**以 0 为中心**，比 sigmoid 函数更好；
- 在 tanh 图中，负输入将被强映射为负，而零输入被映射为接近零。

#### 3. Relu

优点：

- Relu 不耗费资源，导数为1， 运算较快
- 可以解决梯度消失的问题
- ReLU会使一部分神经元的输出为0，这样就造成了网络的稀疏性（类似dropout），并且减少了参数的相互依存关系，缓解了**过拟合**问题的发生。

缺点：

- **Dead ReLU Problem(神经元坏死现象)**：ReLU在负数区域被kill的现象叫做dead relu。ReLU在训练的时很“脆弱”。在x<0时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应，导致相应参数永远不会被更新。可以试试Leaky Relu。

![img](https://pica.zhimg.com/80/v2-8021b9669dc2e92bc2fba49a159abc55_1440w.png)

注：虽然 ReLU 在 0 点不可导，但是它依然存在**左导数和右导数**，只是它们不相等（相等的话就可导了），于是在实现时通常会返回左导数或右导数的其中一个，而不是报告一个导数不存在的错误。所以，虽然Relu 不是全程可微也能用于基于梯度的学习。

#### 4. Leaky Relu

![img](https://pic2.zhimg.com/80/v2-4d592b88fe164d0ca1fdd42a79f8b4a1_1440w.jpg)

#### 5. PRelu (带参数的Leaky Relu)

![img](https://pic2.zhimg.com/80/v2-b8ef68f79d5bc2a2400c72cab4998265_1440w.jpg)



### 0x03. 如何选择激活函数

- 如果是二分类问题， 输出层是sigmoid，其余层是Relu
- 一般隐层采用Relu， 有时也要试试 tanh， 这两大函数的变体都有必要试试

