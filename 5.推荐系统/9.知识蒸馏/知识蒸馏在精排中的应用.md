

# 知识蒸馏在精排中的应用

知识蒸馏主要分为两类：logits蒸馏和中间特征蒸馏。其中，logits蒸馏指的是在softmax时使用较高的温度系数，提升负标签的信息，然后使用Student和Teacher在高温softmax下logits的KL散度作为loss。中间特征蒸馏就是强迫Student去学习Teacher某些中间层的特征。

logit蒸馏：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/zHbzQPKIBPjfBXaIBkO7trKgSchsNviaVzZ8nm8jw84ticcWFLbEWOCibaBCOK3D9tZeyAg4hpfT1nmFzcZmsOEug/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

logits 就是在softmax变成概率值之前的那个原始的输出值。在这里**我们把logits做带温度系数的softmax**。如果把 T 拿掉或令 T=1，则是个标准的 Softmax 公式，zi 就是第 i 个类别的 Logits 数值，qi 是 Logits 数值经过 Softmax 变换后，归属于第 i 个类别的概率值。如果将 T 设大，则 Softmax 之后的 Logits 数值，各个类别之间的概率分值差距会缩小，也即是强化那些非最大类别的存在感；反之，则会加大类别间概率的两极分化。Hinton 版本的知识蒸馏，让 Student 去拟合 Teacher 经过 T 影响后 Softmax 得到的Logits。Student 的损失函数由两项组成，一个子项是 Ground Truth，就是在训练集上的标准交叉熵损失，让 Student 去拟合训练数据，另外一个是蒸馏损失，让 Student 去拟合 Teacher 的 Logits。

中间层蒸馏：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/zHbzQPKIBPjfBXaIBkO7trKgSchsNviaVO9nWCWnIBYibnImXIAdjqO6wicib4bmCXcfRrbUFT3IOMHrU5ECgu3mkg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

它强迫 Student 某些中间层的网络响应，要去逼近 Teacher 对应的中间层的网络响应。这种情况下，Teacher 中间特征层的响应，就是传递给 Student 的暗知识。



为什么要在精排阶段使用知识蒸馏呢？其中一个就是节约机器资源和成本。虽然在精排阶段我们可以使用复杂模型，但是复杂模型的在线服务相应必然变慢（例如xDeepFM在CPU上推理耗时为WDL的2.5~3.5倍，在使用GPU时只有在大batch下，推理性能才符合要求）。所以，我们可以使用复杂的Teacher模型去指导训练简单的Student模型，在线上部署Student模型。另外一个原因是为了充分利用优势特征。所谓优势特征就是那种信号强、但是只能离线获得的特征（例如预测CVR时候的"用户停留时长"特征）。为了充分利用这些优势特征，我们可以使用优势特征训一个Teacher，然后让不使用优势特征的Student学习Teacher的logits，再把Student部署到线上。关于优势特征蒸馏的介绍可以看：[魔法学院的Chilia：KDD 2020 | 优势特征知识蒸馏在淘宝推荐中的应用](https://zhuanlan.zhihu.com/p/422353733)

下面介绍一些有代表性的论文。

### 1. Rocket Launching: A Universal and Efficient Framework for Training Well-performing Light Net

在精排环节采用知识蒸馏，主要采用 Teacher 和 Student **联合训练** ( Joint Learning ) 的方法，而目的是通过复杂 Teacher 来辅导小 Student 模型的训练，将 Student 推上线，增快模型响应速度。

这是阿里在AAAI 2018上发表的一篇论文，属于logits蒸馏。模型结构如下：

![img](https://pic1.zhimg.com/80/v2-f1c16cc9a1e266abe7f0b0c59ea5d874_1440w.jpg)

其中，简单的网络称为轻量网络(Light Net), 复杂的网络称为助推器网络(Booster Net)。和传统的logits蒸馏一样，在训练阶段，Light Net学习Booster Net的soft-target来模仿Booster Net的学习过程；在测试阶段，只使用Light Net来上线预测。那么，这个模型比传统logits蒸馏的改进是什么呢？

**（1）共享参数**

上图黄色部分的参数 ![[公式]](https://www.zhihu.com/equation?tex=W_s) 是共享的，这样Booster Net可以直接更新共享参数，于是Light Net可以直接得到Booster Net的“助推”（"get direct thrust from the booster"）。共享的是低层的参数，这是因为低层参数主要学习的是信息表示、反应的是对输入信息基本的刻画（如embedding层），低层参数共享可以帮助Light Net学习**更好的特征表示**。

**（2）同时训练**

一般的知识蒸馏都是先训练出一个好的Teacher，然后再从Teacher中"蒸馏"出Student模型。但是这篇文章是同时训练Light Net和Booster Net的。在训练阶段，Booster Net和Light Net一起前行；在测试阶段，Booster Net剥离，Light Net独自前行 -- 这就是题目中"Rocket Launching"的由来。

那么，为什么要这么做呢？一方面，Booster Net**全程**提供soft target信息给轻量网络，从而**指导Light Net的整个求解过程**。

> The light model can learn from not only the difference between the target and its temporary outputs, but also the possible **path** towards the final target provided by a complex model with more learning capability.

训练时，网络的loss为：

![img](https://pic4.zhimg.com/80/v2-83f24af2e6534af78e5e61a03bf930bb_1440w.jpg)

loss包含三部分：

- Light Net对true label的交叉熵损失；
- Booster Net对true label的交叉熵损失；
- 两个网络在softmax之前的logits的MSE（hint loss）

另一方面，同时训练可以**缩短训练时间**。

**（3）梯度屏蔽 (gradient block)**

在训练过程中，hint loss只能用于Light Net的梯度更新、不能用于Booster Net的梯度更新。Booster Net只能从true label中学习知识。也就是说，学生可以向老师学，但是老师不能跟学生学，否则就会被学生拖累。

梯度传播图如下：

![img](https://pic4.zhimg.com/80/v2-3f3c878b71871a6e3987358621287083_1440w.jpg)

后记：2020年爱奇艺公布的排序阶段知识蒸馏模型和这个模型十分相似。爱奇艺公布的模型如下：

![img](https://pic3.zhimg.com/80/v2-f7f0cd3756e8a8d5f4dbc5c12fcbd5a6_1440w.jpg)

如上图所示，Student 和 Teacher **共享**特征 Embedding 参数层，Student 模型在损失函数中加入了拟合 Teacher 输出阶段的 Logits 子项，这两点和 rocket launching 是类似的。主要改进有两点：首先，为了进一步增强 student 的泛化能力，要求 student 的**隐层 MLP 的激活也要学习 Teacher 对应隐层的响应（中间层蒸馏）**，这点同样可以通过在 student 的损失函数中加子项来实现。但是这会带来一个问题，就是在 **MLP 隐层复杂度方面，Student 和 Teacher 是相当的**，我们说过，一般知识蒸馏，老师要比学生博学，那么，在这个结构里，Teacher 相比 student，模型复杂在哪里呢？这引出了第二点不同：双 DNN 排序模型的 Teacher 在特征 Embedding 层和 MLP 层之间，可以比较灵活加入各种不同方法的**特征组合**功能，通过这种方式，体现 Teacher 模型的较强的模型表达和泛化能力。

爱奇艺给出的数据对比说明了，这种模式学会的 Student 模型，线上推理速度是 Teacher 模型的5倍，模型大小也缩小了2倍。Student 模型的推荐效果也比 rocket launching 更接近 Teacher 的效果，这说明改进的两点对于 Teacher 传授给 Student 更强的知识起到了积极作用。

### 2. Ensembled CTR Prediction via Knowledge Distillation

这篇华为在CIKM2020上发表的论文，使用了logits蒸馏+中间层特征蒸馏的方法。

读过上面的知识蒸馏基础便可以知道，想要把Teacher学到的知识迁移到Student上，有两种方法。一种是logits蒸馏，即在softmax的时候加一个**较高的温度系数**，来提高负label的概率，获得更多的知识；一种是特征蒸馏，即最小化Teacher和Student**中间层**的差异。有了Teacher带来的更丰富的知识，学生就能够学的比只用true label好得多。"even a simple DNN model can learn well given useful guidance from a good teacher" 。如下图：

![img](https://pic2.zhimg.com/80/v2-5ffbde648d67248eaa80c2047a5c164d_1440w.jpg)

上面所说的是只有一个Teacher的情况。我们知道，模型ensemble在机器学习中经常能取得更好的效果。但是如果在线上使用多个精排模型同时预测并不太现实，因为这样的话训练和推理所耗费的资源就要翻倍了。那么能不能从多个Teacher中一起蒸馏呢？

一种直接的想法是，直接把多个Teacher的logits层/中间层做个平均，然后让Student学习这个已经融合好的单一模型。但是，并不是所有Teacher模型都应该具有同等的权重。毕竟，如果一个Teacher学的很差，就会对整体产生干扰。所以，文中采用了**自适应分配注意力权重**的方法，是用Gate Net实现的：

![img](https://pic3.zhimg.com/80/v2-1a655b22960f90c25e9664dc3a73d78e_1440w.jpg)

计算Student和Teacher的差异公式为：

![img](https://pic2.zhimg.com/80/v2-4fcb78abb18081517f3a88bd52b298c9_1440w.jpg)

其中，注意力权重的分配公式为：

![img](https://pic2.zhimg.com/80/v2-98d9bd6a8e418938a414fedce41ab5f5_1440w.jpeg)

其中 ![[公式]](https://www.zhihu.com/equation?tex=z_%7BT_i%7D) 是第 ![[公式]](https://www.zhihu.com/equation?tex=T_i) 个Teacher的logits或者中间层向量。这样，我们就得到了要分配给每个Teacher的权重。

在训练时，可以使用Teacher和Student一起训练的方法，这样训练的速度更快，但是需要把所有Teachers和Student都load进GPU，对GPU资源要求高。也可以选择"先训练好Teacher、再从Teacher中蒸馏Student"的方法。